---
title: "Social cues modulate the representations underlying cross-situational learning"
output: kmr::els_manuscript
bibliography: "soc-xsit.bib"
csl: apa6.csl
document-params: "authoryear, review"
journal: "Cognitive Psychology"

author-information:
    # Group authors per affiliation:
    - \author[km]{\corref{cor}Kyle MacDonald}
    - \cortext[cor]{Corresponding author}
    - \ead{kyle.macdonald@stanford.edu}
    - \author[dy]{Daniel Yurovsky}
    - \author[mcf]{Michael C. Frank}
    - \address{Department of Psychology, Stanford University, United States}

abstract: 
    "Because children hear language in environments that contain many things to talk about, learning the meaning of even the simplest word requires making inferences under undertainty. A cross-situational statistical learner can aggregate across naming events to form stable word-referent mappings, but this approach neglects an important source of information that can reduce referential uncertainty: social cues from speakers. In three large-scale experiments with adults, we test the effects of varying referential uncertainty in cross-situational word learning using social cues. Social cues shifted learners away from tracking multiple hypotheses and towards storing only a single hypothesis (Experiments 1 and 2). In addition, learners were sensitive to graded changes in the strength of a social cue, and when it became less reliable, they were more likely to store multiple hypotheses (Experiment 3). Our data suggest that the representations underlying cross-situational word learning are quite flexible: In conditions of greater uncertainty, learners tend to store a broader range of information."
    
keywords:
    "statistical learning, social cues, word learning, language acquisition"
---

```{r global_options, include=FALSE}
rm(list=ls())
knitr::opts_chunk$set(fig.width=4.5, fig.height=5, fig.crop = F, fig.path='figs/',
                      echo=FALSE, warning=FALSE, cache=T, message=FALSE, sanitize = T)
```

```{r libraries}
source("../../../analysis/final/useful.R")
#devtools::install_github("kemacdonald/kmr") # uncomment to install, necessary for knitting
library(kmr)
library(pander)
library(magrittr)
library(tidyr)
library(xtable)
```

```{r data}
df_expt1 <- read.csv("../data/final/soc-xsit-expt1-finalData.csv")
df_expt2 <- read.csv("../data/final/soc-xsit-expt2-finalData.csv")
df_expt3 <- read.csv("../data/final/soc-xsit-expt3-finalData.csv")
```

```{r chance_functions}
# Logit function for passing chance performance offsets to glms
logit <- function(x) {log(x/(1-x))}

# Fits logistic regressions to estimate coefficients and significance values 
# for individual factors
test.chance <- function(data,groups,formula,row=1) {
  chance.tests <- data %>%
    group_by_(.dots = groups) %>%
    do(chance.lm = summary(glm(formula, offset=logit(1/numPicN),
                               family="binomial", data = .)))
  
  chance.tests$betas<- sapply(chance.tests$chance.lm,
                                function(x) {x$coefficients[row,1]})
  chance.tests$zs <- sapply(chance.tests$chance.lm,
                            function(x) {x$coefficients[row,3]})
  chance.tests$ps <- sapply(chance.tests$chance.lm,
                            function(x) {x$coefficients[row,4]})
  
  return(select(chance.tests, -chance.lm))
  }
```

```{r stars}
## get stars for significance testing
getstars <- function(x) {
  if (x > .1) {return("")}
  if (x < .001) {return("***")}
  if (x < .01) {return("**")}
  if (x < .05) {return("*")}
  return(".")}
```

```{r descriptives}
# experiment 1
nsubs_expt1 <- df_expt1 %>%
    group_by(condition, intervalNum, numPicN, dataset) %>%
    summarise(n_subs = n_distinct(subids)) %>% 
    mutate(n_excluded = ifelse(condition == "Social" & numPicN == 4, 
                               100 - n_subs, 
                               50 - n_subs))

final_n_expt1 <- sum(nsubs_expt1$n_subs)
nsubs_excluded_expt1 <- sum(nsubs_expt1$n_excluded)

# experiment 2
nsubs_expt2 <- df_expt2 %>% 
    group_by(condition, interval) %>%
    summarise(n_subs = n_distinct(subids)) %>% 
    mutate(n_excluded = 100 - n_subs)

final_n_expt2 <- sum(nsubs_expt2$n_subs)
nsubs_excluded_expt2 <- sum(nsubs_expt2$n_excluded)

# experiment 3
nsubs_expt3 <- df_expt3 %>%
    group_by(prop_cond_clean) %>%
    summarise(n_subs = n_distinct(subids)) %>% 
    mutate(n_excluded = 150 - n_subs)

final_n_expt3 <- sum(nsubs_expt3$n_subs)
nsubs_excluded_expt3 <- sum(nsubs_expt3$n_excluded)
```

# Introduction

Learning the meaning of a new word should be hard. Consider that even concrete nouns are often used in complex contexts with multiple possible referents, which in turn have many conceptually natural properties that a speaker could talk about. This ambiguity creates the potential for an (in principle) unlimited amount of referential uncertainty in the learning task.^[This problem is a simplified version of Quine's \textit{indeterminacy of reference} [@quine19600]: That there are many possible meanings for a word ("Gavigai") that include the referent ("Rabbit") in their extension, e.g., "white," "rabbit," "dinner." Quine's broader philosophical point was that different meanings ("rabbit" and "undetached rabbit parts") could actually be extensionally identical and thus impossible to tease apart.] Remarkably, word learning proceeds despite this uncertainty, with estimates of adult vocabularies ranging between 50,000 to 100,000 distinct words [@bloom2002children]. How do learners infer and retain such a large variety of word meanings from data with this kind of ambiguity?

Statistical learning theories offer a solution to this learning problem by aggregating cross-situational statistics across labeling events to identify underlying word meanings [@yu2007rapid; @siskind1996computational]. Recent experimental work shows that both adults and young infants can use word-object co-occurrence statistics to learn words from individually ambiguous naming events [@smith2008infants; @vouloumanos2008fine]. For example, @smith2008infants taught 12-month-olds three novel words simply by repeating consistent novel word-object pairings across 10 ambiguous exposure trials. Moreover, computational models suggest that cross-situational learning can scale up to learn adult-sized lexicons, even under conditions of considerable referential uncertainty [@smith2011cross].

Although all cross-situational learning models agree that the input is the co-occurrence between words and objects and the output is stable word-object mappings, they disagree about how closely learners approximate the input distribution (for review, see Smith, Suanda, & Yu 2014). One approach is to model learning as a process of updating connection strengths between multiple word-object links [@mcmurray2012word], while other approaches argue that learners store only a single word-object link [@trueswell2013propose]. Recent experimental and modeling work by @yurovsky2014algorithmic suggests an integrative explanation: Learners allocate a fixed amount of their attention to one hypothesis, and the rest gets distributed evenly among the remaining alternatives. As the set of alternatives grows, the amount allocated to each object approaches zero.

In addition to the debate about representation, researchers also disagree about how to best characterize the ambiguity of the input to cross-situational learning mechanisms. One way researchers have quantified this ambiguity is to ask adults to guess the meaning of an intended referent from clips of caregiver-child interactions (Human Simulation Paradigm: HSP). Using the HSP, @medina2011words found that adults did not aggregate multiple word–referent correspondences across trials, concluding that real world learning contexts are too noisy to support tracking of multiple word-object links. In contrast, @yurovsky2013statistical found a bimodal distribution, with half of the naming episodes being unambiguous to adults and half being quite clear. @cartmill2013quality also showed that the proportion of unambiguous naming episodes varies across parents, with some parents’ rarely providing highly informative contexts and others’ doing so relatively often. 

Thus, representations in cross-situational word learning can appear distributional or discrete, and the input to statistical learning mechanisms can vary along a continuum from low to high ambiguity. These results raise an interesting question: could learners be sensitive to the ambiguity of the input and use this information to flexibly alter the representations they store in memory? In the current line of work, we investigate how the presence of referential cues in the social context might alter the ambiguity of the input to statistical word learning mechanisms.

Social-pragmatic theories of language acquisition emphasize the importance of social cues for word learning [@bloom2002children; @clark2009first; @hollich2000breaking]. Experimental work shows that even children as young as 16 months are sophisticated intention-readers, preferring to map novel words to objects that are the target of a speaker’s gaze and not their own [@baldwin1993infants]. In naturalistic observations, learners tend to retain labels that are accompanied with clear referential cues that are concurrent with visual access [@yu2012embodied]. And correlational data show strong links between early intention-reading skills (e.g., gaze following) and later vocabulary growth [@brooks2005development; @brooks2008infant; @carpenter1998social]. Moreover, research outside the domain of language acqusition shows that the presence of social cues: (a) produces better spatial learning of audiovisual events [@wu2011infants], (b) boosts recognition of a cued object [@cleveland2007joint], and (c) leads to preferential encoding of an object’s featural information [@yoon2008communication]. Together, the evidence suggests that social cues could help learners by allowing for efficient allocation of attention to the relevant statistics in the input, and thus change the representations stored in memory. 

In the studies reported here, we ask whether the presence of a valid social cue, a speaker's gaze, changes the representations underlying cross-situational word learning. We use a modified version of @yurovsky2014algorithmic's paradigm, which we describe in greater depth below, to provide a direct measure of memory for alternative word-object links during cross-situational learning. In Experiment 1, we manipulate the presence of a referential cue at different levels of attention and memory demands. At all levels of difficulty, learners tracked a strong single hypothesis, but learners were less likely to track multiple word-object links when a referential cue was present. In Experiment 2, we replicate the findings from Experiment 1 with a more ecologically valid social cue. In Experiment 3, we show that learners are sensitive to graded changes in the reliability of a referential cue and will flexibly increase the number of word-object links they store in response to changes in the quality of the input. In sum, the data suggest that cross-situational word learners are quite flexible, storing representations with different levels of fidelity depending on the amount of ambiguity present during learning.	

# Experiment 1 

We set out to test the effect of a referential cue on the representations underlying cross-situational word learning. We use a version of @yurovsky2014algorithmic's paradigm where we manipulate the ambiguity of the learning context by including a gaze cue from a schematic, female interlocutor. Participants saw a series of ambiguous exposure trials where they heard one novel word that was either paired with a gaze cue or not and selected the object they thought went with each word. In subsequent test trials, participants heard the novel word again, this time paired with a new set of novel objects. One of the objects in this set was either the participant's initial guess (Same test trials) or one of the objects was *not* their initial guess (Switch test trials). Performance on Switch trials provides a direct measure of whether referential cues influenced the number of alternative word-object links that learners stored in memory. If learners perform worse on Switch trials after an exposure trial with gaze, this suggests that they stored fewer additional objects from the less ambiguous initial learning context.

## Method

### Participants

We posted a set of Human Intelligence Tasks (HITs) to Amazon Mechanical Turk. Only participants with US IP addresses and a task approval rate above 95% were allowed to participate, and each HIT paid 30 cents. 50-100 HITs were posted for each of the 32 between-subjects conditions. Data were excluded if participants completed the task more than once or if participants did not respond correctly on familiar object trials (`r nsubs_excluded_expt1` HITs). The final sample consisted of 1,523 participants. 

```{r stimuli, fig.pos = "tb", fig.width=4.5, fig.height=3.5, fig.cap = "Screenshots of exposure and test trials from Experiment 1 (schematic gaze cue) and Experiments 2 \\& 3 (human actress gaze cue). Participants saw exposure trials with or without a gaze cue depending on condition assignment. All participants saw both types of test trials: Same and Switch. On Same trials the object that participants chose during exposure appeared with a new novel object. On Switch trials the object that participants did not choose appeared with a new novel object."}

grid::grid.raster(png::readPNG("figs/stimuli.png"))
```

### Stimuli

Figure 1 shows screenshots taken from Experiment 1. Visual stimuli were black and white pictures of familiar and novel objects taken from @kanwisher1997locus. Auditory stimuli were recordings of familiar and novel words by an AT&T Natural Voices \texttrademark (voice: Crystal) speech synthesizer. Novel words were 1-3 syllable pseudowords that obeyed all rules of English phonotactics. A schematic drawing of a human speaker was chosen for ease of manipulating the direction of gaze, the referential cue of interest in this study. All experiments can be viewed and downloaded at the project page: https://kemacdonald.github.io/soc_xsit/.

### Design and Procedure

Participants saw a total of 16 trials: eight exposure trials and eight test trials. On each trial, they heard one novel word, saw a set of novel objects, and were asked to guess which object went with the word. Before seeing exposure and test trials, participants completed four practice trials with familiar words and objects. These trials familiarized participants to the task and allowed us to exclude participants who were unlikely to perform the task as directed either because of inattention or because their computer audio was turned off. 

After the practice trials, participants were told that they would now hear novel words and see novel objects, and that their task was to select the referent that "goes with each word." Over the course of the experiment, participants heard eight novel words two times, with one exposure trial and one test trial for each word. Four of the test trials were *Same* trials in which the object that participants selected on the exposure trial was shown with a set of new novel objects. The other four test trials were *Switch* trials in which one of the objects was chosen at random from the set of objects that the participant did not select on exposure. 

Participants were randomly assigned to the 32 between-subjects conditions (4 Referents X 4 Intervals X 2 Gaze). Participants either saw 2, 4, 6, or 8 referents on the screen and test trials occurred after either 0, 1, 3, or 7 trials from the initial exposure to a word. Participants were assigned to the Gaze or No-gaze conditions. In the Gaze condition, gaze was directed towards one of the objects on exposure trials; in the No-gaze condition, gaze was always directed straight ahead (see Figure 1 for examples of these trial types). At test, gaze was never informative. To show participants that their response had been recorded, a red box appeared around the selected object for one second. This box always appeared around the selected object, even if participants' selections were incorrect.

## Results and Discussion

### Analysis plan

The structure of our analysis plan is parallel across all three experiments. First, we examine performance on Exposure trials to provide evidence that learners were (a) sensitive to our experimental manipulation and (b) altered their allocation of attention in response to changes in contextual ambiguity. Then we examine performance on Test trials to show that learners' memory for alternative word-object links changes depending on the ambiguity of the learning context. The key behavioral prediction of our hypothesis is that the presence of gaze will result in reduced memory for multiple word-object links, operationalized as a decrease in performance on Switch test trials after seeing Exposure trials with a gaze cue. To quantify participants' behavior, we use mixed effects regression models with the maximal random effects structure justified by our experimental design: by-subject intercepts and slopes for each trial type. All mixed-effects models were fit using the lme4 package in R [@bates2013lme4], and all of our data, processing, and analysis code can be viewed in the version control repository for this paper at: https://github.com/kemacdonald/soc_xsit.

```{r expt1-plot, fig.pos = "tb", fig.width=4.5, fig.height=5, fig.cap = "Experiment 1 results. Panel A shows response times on exposure trials across all experimental conditions: Gaze and No-gaze, Referents (2, 4, 6, and 8), and Intervening trials (0, 1, 3, and 7). Panel B shows accuracy on test trials for Same and Switch trials across all conditions. The horizontal dashed lines represent chance performance for each condition. Colored lines are linear model fits and error bars indicate 95\\% confidence intervals computed by non-parametric bootstrap."}
grid::grid.raster(png::readPNG("figs/expt1.png"))
```

### Exposure trials

```{r expt1_chance_exposure}
e1.chance.tests.exposure <- test.chance(data = filter(df_expt1, trial_category == "exposure", 
                                       include_good_rt_exposure == "include", include_expo == "include",
                                       condition == "Social"),
                               groups = c("numPicN","intervalNum"),
                               formula = as.formula("correct_exposure ~ 1")) %>%
  mutate(stars = getstars(ps))
```

```{r rt exposure expt1 lmer}
m1_rt_expt1 <- lmer(rt ~ condition * log2(intervalNum + 1) * log2(numPicN) + (1|subids), 
                    data=filter(df_expt1, trial_category == "exposure", 
                                include_good_rt_exposure == "include"))

beta_referents <- round(summary(m1_rt_expt1)$coef[4], 2)
beta_gaze_referents <- round(summary(m1_rt_expt1)$coef[6], 2)
```

To ensure that our referential cue manipulation was effective we compare participant's accuracy \footnote{Correct performance is defined as selecting the object that was the target of the speaker's gaze.} on Exposure trials in the Gaze condition to a model of random behavior defined as a Binomial distribution with a probability of success $\frac{1}{Num Referents}$. Following @yurovsky2014algorithmic, we fit logistic regressions for each Gaze, Referent, and Interval combination specified as \texttt{Correct $\sim$ 1 + offset(logit(1/Referents))}. The offset encodes the chance probability of success given the number of referents, and the coefficient for the intercept term shows on a log-odds scale how much more likely participants are to select the gaze target than would be expected if participants were selecting randomly. In all conditions participants used gaze to select referents on Exposure trials more often than expected by chance (smallest $\beta$ = `r round(sort(e1.chance.tests.exposure$betas)[1], 2)`, z = `r round(sort(e1.chance.tests.exposure$zs)[1], 2)`, p < .001).

We were also interested in differences in participants' response times across the experimental conditions. Since these trials were self-paced, participants could choose how much time to spend studying the referents on the screen, thus providing an index of participants' attention. To quantify the effects of gaze, interval, and number of referents, we fit a linear mixed effects model predicting participants' response times as follows: \texttt{RT $\sim$ Gaze Condition * Log(Interval) * Log(Referents) + (1 | subject)}. We found a significant main effect of referents ($\beta$ = `r beta_referents`, p < .001) with slower responses as the number of referents increased, and a significant two-way interaction between Gaze condition and number of referents ($\beta$ = `r beta_gaze_referents`, p < .001) such that responses were faster in the Gaze condition, especially as the number of referents increased. The interaction between Gaze condition and number of referents is shown in Panel A of Figure 2. Faster response times on Exposure trials with gaze provides preliminary evidence that the presence of a referential cue focused participants' attention on the gaze target and away from alternative word-object links. 

### Test trials

```{r expt1_chance}
e1.chance.tests <- test.chance(data = filter(df_expt1, trial_category == "test", 
                                       include_good_rt_test == "include",
                                       include_expo == "include" | condition == "No-Social",
                                       correct_exposure == T | condition == "No-Social"),
                               groups = c("trialType","numPicN","intervalNum"),
                               formula = as.formula("correct ~ 1")) %>%
  mutate(stars = getstars(ps))
```

```{r expt1 glmer}
m1_2way_acc_expt1 <- glmer(correct ~ (trialType + condition + log2(intervalNum + 1) + log2(numPicN))^2 + 
                               (trialType | subids), offset = logit(1/numPicN), 
                           control=glmerControl(optimizer="bobyqa"),
                           family=binomial, 
                           nAGQ=0,
                           data=filter(df_expt1, trial_category == "test", 
                                       include_good_rt_test == "include",
                                       include_expo == "include" | condition == "No-Social",
                                       correct_exposure == T | condition == "No-Social"))

m1_3way_acc_expt1_filt <- glmer(correct ~ (trialType + condition + log2(intervalNum + 1) + log2(numPicN))^3 +
                                (trialType | subids), offset = logit(1/numPicN),
                                control=glmerControl(optimizer="bobyqa"),
                                family=binomial, 
                                nAGQ=0, 
                                data=filter(df_expt1, trial_category == "test", 
                                            include_good_rt_test == "include", 
                                            include_expo == "include" | condition == "No-Social",
                                            correct_exposure == T | condition == "No-Social"))
                                
                                
# two-way main effects
beta_trialtype_acc <- round(summary(m1_2way_acc_expt1)$coef[2], 2)
beta_gaze_acc <- round(summary(m1_2way_acc_expt1)$coef[3], 2)
beta_interval_acc <- round(summary(m1_2way_acc_expt1)$coef[4], 2)
beta_referents_acc <- round(summary(m1_2way_acc_expt1)$coef[5], 2)

# two-way interactions
beta_gaze_trialtype_int <- round(summary(m1_2way_acc_expt1)$coef[6], 2)
beta_trialtype_interval_int <- round(summary(m1_2way_acc_expt1)$coef[7], 2)
beta_trialtype_referents_int <- round(summary(m1_2way_acc_expt1)$coef[8], 2)
beta_gaze_referents_int <- round(summary(m1_2way_acc_expt1)$coef[10], 2)

# three-way interaction
beta_gaze_trialtype_3way_int <- round(summary(m1_3way_acc_expt1_filt)$coef[12], 2)
```

Panel B of Figure 2 shows participants' accuracies in identifying the referent for each word in all conditions for both kinds of trials (Same and Switch). We first compared the distribution of correct responses made by each participant to the distribution expected if participants were selecting randomly defined as a Binomial distribution with a probability of success $\frac{1}{Num Referents}$. We fit the same logistic regressions as we did for Exposure trials: \texttt{Correct $\sim$ 1 + offset(logit(1/Referents))}. On 31 out of the 32 conditions for both Same and Switch trials, participants chose the correct target more often than would be expected by chance (smallest $\beta$ = `r round(sort(e1.chance.tests$betas)[2], 2)`, z = `r round(sort(e1.chance.tests$zs)[2], 2)`, p = `r round(sort(e1.chance.tests$ps, decreasing = T)[2], digits = 2)`). On Switch trials in the 8 referent, 3 interval condition, participants' responses were not significantly different from chance ($\beta$ = `r round(sort(e1.chance.tests$betas)[1], 2)`, z = `r round(sort(e1.chance.tests$zs)[1], 2)`, p = `r round(sort(e1.chance.tests$ps, decreasing = T)[1], digits = 2)`). Participants' success on Switch trials replicates the findings from @yurovsky2014algorithmic and provides direct evidence that learners encoded more than a single hypothesis in ambiguous word learning situations, even under high attentional and memory demands, and even in the presence of a referential cue.

```{r expt1-table, echo = F, results = 'asis'}
e1.tab <- as.data.frame(summary(m1_2way_acc_expt1)$coef)

e1.tab$Predictor <- c("Intercept",
                      "Switch Trial",
                      "Gaze Condition",
                      "Log(Interval)",
                      "Log(Referents)",
                      "Switch Trial*Gaze Condition",
                      "Switch Trial*Log(Interval)",
                      "Switch Trial*Log(Referent)",
                      "Gaze Condition*Log(Interval)",
                      "Gaze Condition*Log(Referent)",
                      "Log(Interval)*Log(Referent)")

rownames(e1.tab) <- NULL
e1.tab <- e1.tab[,c(5,1:4)]
names(e1.tab)[4:5] <- c("$z$ value","$p$ value")

e1.tab %<>% 
    mutate(
        stars = ifelse(`$p$ value` > .1, "", 
                       ifelse(`$p$ value` < .001, "***",
                              ifelse(`$p$ value` < .01, "**",
                                     ifelse(`$p$ value` < .05, "*",
                                            ifelse(`$p$ value` < .1, ".", "Error"))))),
        `$p$ value` = ifelse(`$p$ value` > .1, round(`$p$ value`, 2), 
                             ifelse(`$p$ value` < .001, "$<$ .001",
                                    ifelse(`$p$ value` < .01, round(`$p$ value`, 2),
                                           ifelse(`$p$ value` < .05, round(`$p$ value`, 2),
                                                  ifelse(`$p$ value` < .1, round(`$p$ value`, 2), 
                                                         "Error")))))
    )

names(e1.tab)[6] <- c("")

print(xtable(e1.tab,
             align = c("l","l","r","r","r","r","l"),
             label = "tab:exp1_reg",
             caption = "Predictor estimates with standard errors and significance information for a logistic mixed-effects model predicting word learning in Experiment 1."),
      include.rownames=FALSE,hline.after=c(0,nrow(e1.tab)),
      sanitize.text.function=function(x){x},
      caption.placement = 'bottom', 
      table.placement = "tb",
      comment = F)
```

To quantify the effect of each predictor on the probability of a correct response, we fit the following mixed-effects logistic regression model to a filtered dataset, removing participants who were not reliably selecting the referent that was the target of gaze on exposure trials:\footnote{We did not predict that there would be a subset of participants who would not follow the gaze cue, thus this filtering criteria was developed post-hoc. However, we believe the filter is theoretically motivated because we would only expect to see an effect of gaze if participants were actually using the gaze cue. The filter removes 90 participants who did not reliably select the gaze target on exposure trials. The key inferences from the data do not depend on this filtering criteria.} \texttt{Correct $\sim$ Trial Type * Gaze + Trial Type * Log(Interval) + Trial Type * Log(Referents) + \\ offset(logit($^1/_{Referents}$)) + (TrialType | subject)}. 
We follow @yurovsky2014algorithmic's analysis plan and coded interval and number of referents as continuous predictors and transformed these variables to the log scale. We limited the model to include only two-way interactions because the critical test of our hypothesis is the interaction between Gaze condition and Trial Type, and we did not have any theoretical predictions for possible three-way interactions.\footnote{If we allow for three-way interactions in the model, there is a significant interaction between Gaze condition, Trial Type, and Interval ($\beta = `r beta_gaze_trialtype_3way_int`$, p $<$ .01). The two-way interaction betwen Gaze condition and Trial Type remains significant in this more complex model. A model including four-way interactions did not sufficiently improve model fit in order to justify the added complexity.}

Table 1 shows the output of the logistic regression. We found significant main effects of Referents ($\beta = `r beta_referents_acc`$, p < .001) and Interval ($\beta = `r beta_interval_acc`$, p < .001), such that as each of these factors increased, accuracy on test trials decreased. We also found significant main effects of Trial Type ($\beta = `r beta_trialtype_acc`$, p < .001), with worse overall performance on Switch trials. There were significant interactions between Trial Type and Interval ($\beta = `r beta_trialtype_interval_int`$, p < .001), Trial Type and Referents ($\beta = `r beta_trialtype_referents_int`$, p < .001), and Gaze condition and Referents ($\beta = `r beta_gaze_referents_int`$, p < .05). These interactions can be interpreted as (a) the interval between exposure and test affecting Same trials more than Switch trials, (b) the number of referents affecting Switch trials more than Same trials, and (c) participants performing slightly better at higher number of referents in the Gaze condition (see Panel B of Figure 2). The interactions between Gaze condition and Referents and between Referents and Interval were not significant. Crucially, we found the predicted interaction between Trial Type and Gaze condition ($\beta = `r beta_gaze_trialtype_int`$, p < .001), with participants in the Gaze condition performing worse on Switch trials. This interaction provides direct evidence that the presence of a referential cue selectively reduced participants' memory for alternative word-object links. 

Taken together, the response time and accuracy analyses provide evidence that the presence of a referential cue modulated learners' attention during learning, and in turn made them less likely to track multiple word-object links. We did not see strong evidence that reduced tracking of alternatives resulted in an increase in performance on Same trials. This finding suggests that the limitations on Same trials may be different than those regulating the distribution of attention on Switch trials, since the presence of a referential cue selectively reduced learners tracking of alternatives but apparently did not lead learners to form a stronger memory of their single candidate hypothesis. 

There was relatively large variation in performance across conditions in group-level accuracy scores and in participants' tendency to *use* the referential cue on exposure trials. Moreover, we found a subset of participants who did not reliably use the gaze cue at all, potentially reducing the effect of gaze on cross-situational learning in this experiment. It is possible that the effect of gaze was reduced because the referential cue that we used -- a static schematic drawing of a speaker -- was relatively weak compared to the cues present in real world learning environments. We do not yet know how learners' memory for alternatives during cross-situational learning would change in the presence of a stronger and more ecologically valid referential cue. Experiment 2 attempts to answer this question. 

# Experiment 2

In Experiment 2, we attempt to replicate the findings from Experiment 1 using a more ecologically valid stimulus set. We replaced the static, schematic drawing with a video of a live actress. While the video stimuli is still far from actual learning contexts, it included a real person who provided both a gaze cue and a head turn towards the target object. To reduce the across-conditions variability, we introduced a within-subjects design where each participant saw both Gaze and No-gaze exposure trials. We selected a subset of conditions from Experiment 1, testing only the 4-referent display with 0 and 3 intervening trials as between-subjects manipulations. Our goals were to replicate the reduction in learners' multiple alternatives tracking in the presence of referential cues, and to test whether increasing the ecological validity of the cue would result in a boost to the strength of learners' recall of their single candidate hypothesis.  

## Method

### Participants

Participant recruitment and inclusionary/exclusionary criteria were identical to those of Experiment 1 (excluded 36 HITs). 100 HITs were posted for each condition (1 Referent X 2 Intervals X 2 Gaze conditions) for total of 400 paid HITs. 

### Stimuli

Audio and picture stimuli were identical to Experiment 1. The referential cue in the Gaze condition was a video (see Figure 1). On each exposure trial, the actress looked out at the participant with a neutral expression, smiled, and then turned to look at one of the four images on the screen. She maintained her gaze for 3 seconds before returning to the center. On test trials, she looked straight ahead for the duration of the trial. 

## Design and Procedure

Procedures were identical to those of Experiment 1. The major design change was a within-subjects manipulation of the gaze cue with each participant seeing exposure trials with and without gaze. The experiment consisted of 32 trials broken down into 2 blocks of 16 trials. Each block consisted of 8 exposure trials and 8 test trials (4 Same trials and 4 Switch trials), and contained only Gaze or No-gaze exposure trials. The order of block was counterbalanced across participants. 

## Results and Discussion

We followed the same analysis plan as in Experiment 1, first analyzing performance on exposure trials, and then analyzing performance on test trials.

### Exposure trials

```{r expt2_chance_exposure}
e2.chance.tests.exposure <- test.chance(data = filter(df_expt2, trial_cat == "exposure", 
                                       include_good_rt == "include", 
                                       condition_trial == "social"),
                               groups = c("numPicN","intervalNum"),
                               formula = as.formula("correct_exposure ~ 1")) %>%
  mutate(stars = getstars(ps))
```

```{r expt2 lmer rt}
m1_rt_expt2 <- lmer(rt ~ condition_trial * log2(intervalNum + 1) + (1|subids), 
                    data=filter(df_expt2, trial_cat == "exposure"))

beta_gaze_rt_expt2 <- round(summary(m1_rt_expt2)$coef[2], 2)
beta_interval_rt_expt2 <- round(summary(m1_rt_expt2)$coef[3], 2)
```

Similar to Experiment 1, participants' responses on exposure trials differed from those expected by chance (smallest $\beta$ = `r round(sort(e2.chance.tests.exposure$betas)[1], 2)`, z = `r round(sort(e2.chance.tests.exposure$zs)[1], 2)`, p < .001), suggesting that gaze was effective in directing attention to the target referent. Participants in Experiment 2 were numerically more consistent in their use of gaze with the live action stimuli compared to the schematic stimuli used in Experiment 1 ($M_1 = .76, M_2 = .81$), suggesting that using a live actress resulted in a slight increase in participants' willingness to follow the gaze cue.

Panel A of Figure 3 shows participants' response times. We replicate the findings from Experiment 1, with faster response times in Gaze condition. We fit a linear mixed effects model to response times with the same specification as Experiment 1, finding main effects for Gaze condition ($\beta$ = `r beta_gaze_rt_expt2`, p < .001) and Interval ($\beta$ = `r beta_interval_rt_expt2`, p < .001) with faster responses in the Gaze condition and in the longer Interval conditions. The two-way interaction between Gaze condition and interval was not significant, with gaze having the same effect on participants' response times at both intervals.

```{r expt2-plot, fig.pos = "tb", fig.width=4.5, fig.height=3.5, fig.cap = "Experiment 2 results. Panel A shows study times for exposure trials with and without gaze. Panel B shows accuracy on test trials for same and Switch trials across all conditions. The dashed line in Panel B represents chance performance. Error bars indicate 95\\% confidence intervals computed by non-parametric bootstrap."}
grid::grid.raster(png::readPNG("figs/expt2.png"))
```

### Test trials

```{r expt2_chance}
e2.chance.tests <- test.chance(data = filter(df_expt2, trial_cat == "test"),
                               groups = c("trialType","numPicN","intervalNum"),
                               formula = as.formula("correct ~ 1")) %>%
  mutate(stars = getstars(ps))
```

```{r expt 2 acc glmer}
m1_acc_expt2 <- glmer(correct ~ (trialType + condition_trial + log2(intervalNum + 1))^2 +
                          (trialType | subids), 
                      nAGQ=0,
                      glmerControl(optimizer = "bobyqa"),
                      family=binomial,
                      data = filter(df_expt2, trial_cat == "test")
                      )

beta_trialtype_expt2 <- round(summary(m1_acc_expt2)$coef[2], 2)
beta_interval_expt2 <- round(summary(m1_acc_expt2)$coef[4], 2)
beta_trialtype_gaze_expt2 <- round(summary(m1_acc_expt2)$coef[5], 2)
beta_trialtype_interval_expt2 <- round(summary(m1_acc_expt2)$coef[6], 2)
beta_interval_gaze_expt2 <- round(summary(m1_acc_expt2)$coef[7], 2)
beta_interval_gaze_expt2_p <- round(summary(m1_acc_expt2)$coef[7,4], 2)
```

```{r expt2-table, echo = F, results = 'asis'}
e2.tab <- as.data.frame(summary(m1_acc_expt2)$coef)

e2.tab$Predictor <- c("Intercept",
                      "Switch Trial",
                      "Gaze Condition",
                      "Log(Interval)",
                      "Switch Trial*Gaze Condition",
                      "Switch Trial*Log(Interval)",
                      "Gaze Condition*Log(Interval)")

rownames(e2.tab) <- NULL
e2.tab <- e2.tab[,c(5,1:4)]
names(e2.tab)[4:5] <- c("$z$ value","$p$ value")

e2.tab %<>% 
    mutate(
        stars = ifelse(`$p$ value` > .1, "", 
                       ifelse(`$p$ value` < .001, "***",
                              ifelse(`$p$ value` < .01, "**",
                                     ifelse(`$p$ value` < .05, "*",
                                            ifelse(`$p$ value` < .1, ".", "Error"))))),
        `$p$ value` = ifelse(`$p$ value` > .1, round(`$p$ value`, 2), 
                             ifelse(`$p$ value` < .001, "$<$ .001",
                                    ifelse(`$p$ value` < .01, round(`$p$ value`, 2),
                                           ifelse(`$p$ value` < .05, round(`$p$ value`, 2),
                                                  ifelse(`$p$ value` < .1, round(`$p$ value`, 2), 
                                                         "Error")))))
    )


names(e2.tab)[6] <- c("")

print(xtable(e2.tab,
             align = c("l","l","r","r","r","r","l"),
             label = "tab:exp2_reg",
             caption = "Predictor estimates with standard errors and significance information for a logistic mixed-effects model predicting word learning in Experiment 2."),
      include.rownames=FALSE,hline.after=c(0,nrow(e2.tab)),
      sanitize.text.function=function(x){x},
      caption.placement = 'bottom', 
      table.placement = "tb",
      comment = F)
```

Panel B of Figure 3 shows performance on test trials in Experiment 2. Across all conditions for both Trial Types participants selected the correct referent at rates greater than chance (smallest $\beta$ = `r round(sort(e2.chance.tests$betas)[1], 2)`, z = `r round(sort(e2.chance.tests$zs)[1], 2)`, p < .001). We replicate the critical finding from Experiment 1: after seeing exposure trials with gaze, participants performed worse on Switch trials, providing evidence that they stored fewer word-object links. We fit a mixed-effects logistic regression model with the same specifications as in Experiment 1 and found significant main effects of Interval ($\beta = `r beta_interval_expt2`$, p < .001) and Trial Type ($\beta = `r beta_trialtype_expt2`$, p < .001). Participants were less accurate as the interval between exposure and test increased and on the Switch trials overall. 

In addition, there was a significant two-way interaction between Trial Type and Interval ($\beta = `r beta_trialtype_interval_expt2`$, p < .001), with worse performance on Switch trials at the higher intervals, and a marginal two-way interaction betwen Gaze condition and Interval ($\beta = `r beta_interval_gaze_expt2`$, p = `r beta_interval_gaze_expt2_p`) such that the number of intervening trials had a smaller effect on participants' performance in the Gaze condition. We found a robust interaction between Gaze condition and Trial Type ($\beta =  `r beta_trialtype_gaze_expt2`$, p < .001) with Switch trials being more difficult after gaze exposure trials.\footnote{As in Experiment 1, we fit this model a filtered dataset removing participants who did not reliably use the gaze cue.} Once again, we did not see evidence of a boost to performance on Same trials in the Gaze condition. 

The results of Experiment 2 provide converging evidence for our hypothesis, showing that the presence of a referential cue reliably focused learners' attention away from alternative word-object links and shifted them towards single hypothesis tracking. Changing to a live action stimulus set led to slightly higher rates of selecting the target of gaze on exposure trials, but did not result in a boost to performance on Same trials. The selective effect of gaze on Switch trials provides additional evidence that the fidelity of participants' single hypothesis was unaffected by the presence of a referential cue in our paradigm. 

Thus far we have shown that people store different amounts of information in response to a categorical manipulation of referential uncertainty. In both Experiments 1 and 2, the learning context was either entirely ambiguous (No-gaze) or entirely unambiguous (Gaze). But not all real world learning contexts fall at the extremes of this continuum (although see Yurovsky et al., 2013). Could learners be sensitive to more subtle changes in the quality of learning contexts? In our next experiment, we test a prediction of our account: whether learners store more word-object links in response to graded changes of referential uncertainty during learning.

# Experiment 3

In Experiment 3, we explore whether learners will allocate attention and memory flexibly in response to *graded* changes in the referential uncertainty present during learning. To test this hypothesis, we move beyond a categorical manipulation of the presence/absence of gaze, and we parametically vary the strength of the referential cue. We manipulate cue strength by including a block of familiarization trials where we vary the proportion of Same and Switch trials. If participants saw more Switch trials, this provides evidence that the speaker's gaze was an unreliable cue to reference. This design was inspired by a growing body of experimental work showing that even young children are sensitive to the prior reliability of speakers and will use this information when deciding whom to learn novel words from [@koenig2004trust]. 

## Method

### Participants

Participant recruitment, and inclusionary/exclusionary criteria were identical to those of Experiment 1 and 2 (excluded 4 HITs). 100 HITs were posted for each reliability level (0%, 25%, 50%, 75%, and 100%) for total of 500 paid HITs.  

### Design and Procedure

Procedures were identical to those of Experiment 1 and 2. We modified our cross-situational learning paradigm to include a block of 16 familiarization trials (8 exposure trials and 8 test trials), which established the reliability of the speaker. To establish reliability, we varied the proportion of Same/Switch trials that occurred during this familiarization block. Recall that on Switch trials the gaze target does not show up at test, thus providing evidence that this speaker's gaze might not be a reliable cue to reference. Gaze reliability was a between-subjects manipulation, with participants either seeing 0, 2, 4, 6, or 8 Switch trials. After the familiarization block, participants completed another block of 16 trials (8 exposure trials and 8 test trials). Since we were no longer testing the effect of the presence or absence of a referential cue, all exposure trials in Experiment 3 included gaze, but this cue was more or less reliable depending on which familiarization block participants saw. Finally, at the end of the task we asked participants to assess the reliability of the speaker on a continuous scale from "completely unreliable" to "completely reliable." 

## Results and Discussion

### Exposure trials

```{r expt3_chance_exposure}
e3.chance.tests.exposure <- test.chance(data = filter(df_expt3, trial_category == "exposure", 
                                                      block == "test", experiment == "replication", 
                                                      include_good_rt == "include"),
                               groups = c("prop_cond_clean"),
                               formula = as.formula("correct ~ 1")) %>%
  mutate(stars = getstars(ps))
```

```{r expt3 lmer acc expo test block}
m1_expo_expt3 <- glmer(correct ~ reliability * rel_subj  + (1 | subids), 
                       offset = logit(1/numPicN),
                       nAGQ = 1,
                       control = glmerControl(optimizer = "bobyqa"),
                       family = binomial,
                       data = filter(df_expt3, trial_category == "exposure", block == "test",
                                     experiment == "replication", include_good_rt == "include")
                       )

beta_rel_exposure_expt3 <- round(summary(m1_expo_expt3)$coef[2], 2)
beta_rel_subj_exposure_expt3 <- round(summary(m1_expo_expt3)$coef[3], 2)
beta_rel_subj_exposure_interaction_expt3 <- round(summary(m1_expo_expt3)$coef[4], 2)

# marginal p
beta_rel_subj_exposure_interaction_expt3_p <- round(summary(m1_expo_expt3)$coef[4,4], 2)
```

```{r gaze following means}
ms_expt3 <- df_expt3 %>% 
    filter(trial_category == "test", block == "test", 
           include_good_rt == "include", experiment == "replication") %>% 
    group_by(prop_cond_clean) %>%
    summarise(accuracy = mean(correct_exposure, na.rm=T),
              ci_low = ci.low(correct_exposure),
              ci_high = ci.high(correct_exposure))
```

Similar to Experiments 1 and 2, participants reliably chose the referent that was the target of gaze at rates greater than those that would be predicted by a guessing model (smallest $\beta$ = `r round(sort(e3.chance.tests.exposure$betas)[1], 2)`, z = `r round(sort(e2.chance.tests.exposure$zs)[1], 2)`, p < .001). To quantify the effect of reliability condition and particpants' subjective reliablity assessment, we fit a mixed effects logistic regression model predicting the probability of selecting the gaze target as follows: \texttt{Correct-Exposure $\sim$ Reliability Condition * Subjective Reliability + offset(logit(1/Referents)) + (1 | subject)}. We found significant main effects of both reliability condition ($\beta$ = `r beta_rel_exposure_expt3`, p < .05) and subjective reliabilty ($\beta$ = `r beta_rel_subj_exposure_expt3`, p < .001) such that when the gaze cue was more reliable and when subjective reliability assessments were higher participants were more likely to use the gaze cue. The interaction between speaker reliability and subjective reliablity assessments was marginally significant ($\beta$ = `r beta_rel_subj_exposure_interaction_expt3`, p = `r beta_rel_subj_exposure_interaction_expt3_p`). This analysis provides evidence that participants were sensitive to the reliability manipulation both in how often they used the gaze cue during the task and in how they rated the speaker at the end of the task.

```{r expt3-plot, fig.pos = "tb", fig.width=4.5, fig.height=4, fig.cap = "Accuracy on test trials in Experiment 3 for both same and switch trial types. Panel A shows accuracy as a function of the speaker's reliability. Panel B shows accuracy as a function of participants' gaze following on exposure trials. Panel C shows accuracy as a function of participants' subjective reliability judgments, grouped into five equally spaced bins. The horizontal dashed line represents the expected performance if participants were selecting randomly. The colored lines are linear model fits and error bars indicate 95\\% confidence intervals computed by non-parametric bootstrap."}
grid::grid.raster(png::readPNG("figs/expt3.png"))
```

### Test trials

```{r expt3_chance}
e3.chance.tests <- test.chance(data = filter(df_expt3, trial_category == "test", block == "test",
                                     experiment == "replication", include_good_rt == "include"),
                               groups = c("trialType","prop_cond_clean"),
                               formula = as.formula("correct ~ 1")) %>%
  mutate(stars = getstars(ps))
```

```{r glmer condition expt3}
m1_expt3 <- glmer(correct ~ trialType * reliability + (trialType | subids), 
                  offset = logit(1/numPicN),
                  nAGQ = 1,
                  family = binomial,
                  data = filter(df_expt3, trial_category == "test", block == "test",
                                     experiment == "replication", include_good_rt == "include"),
                       control = glmerControl(optimizer = "bobyqa")
                  )

beta_trialtype_int_expt3 <- round(summary(m1_expt3)$coef[3], 2)
beta_rel_trialtype_int_expt3 <- round(summary(m1_expt3)$coef[4], 2)
beta_rel_trialtype_int_p_expt3 <- round(summary(m1_expt3)$coef[4,4], 2)
```

```{r glmer original condition expt3}
m1_expt3_original <- glmer(correct ~ reliability * trialType + (trialType | subids),
                           offset = logit(1/numPicN),
                           control = glmerControl(optimizer = "bobyqa"),
                           nAGQ = 1,
                           family = binomial,
                           data = filter(df_expt3, trial_category == "test", block == "test",
                                experiment == "original", include_good_rt == "include")
                           )

beta_rel_expt3_original <- round(summary(m1_expt3_original)$coef[2], 2)
beta_rel_expt3_original <- round(summary(m1_expt3_original)$coef[3], 2)
beta_rel_trialtype_int_expt3_original <- round(summary(m1_expt3_original)$coef[4], 2)
```

```{r glmer total exposure correct expt3}
m2_expt3 <- glmer(correct ~ total_exposure_correct * trialType + (trialType | subids),
                  offset = logit(1/numPicN),
                  control = glmerControl(optimizer = "bobyqa"),
                  nAGQ = 1,
                  family = binomial,
                  data = filter(df_expt3, trial_category == "test", block == "test",
                                experiment == "replication", include_good_rt == "include")
                  )

beta_totexpo_expt3 <- round(summary(m2_expt3)$coef[2], 2)
beta_trialtype_expt3 <- round(summary(m2_expt3)$coef[3], 2)
beta_totexpo_trialtype_int_expt3 <- round(summary(m2_expt3)$coef[4], 2)
```

```{r glmer original total exposure correct expt3}
m2_expt3_original <- glmer(correct ~ total_exposure_correct * trialType + (trialType | subids),
                           offset = logit(1/numPicN),
                           control = glmerControl(optimizer = "bobyqa"),
                           nAGQ = 1,
                           family = binomial,
                           data = filter(df_expt3, trial_category == "test", block == "test",
                                experiment == "original", include_good_rt == "include")
                           )

beta_totexpo_expt3_original <- round(summary(m2_expt3_original)$coef[2], 2)
beta_trialtype_expt3_original <- round(summary(m2_expt3_original)$coef[3], 2)
beta_totexpo_trialtype_int_expt3_original <- round(summary(m2_expt3_original)$coef[4], 2)
```

```{r glmer subjective reliability expt 3}
m3_expt3 <- glmer(correct ~ rel_subj * trialType + (trialType | subids),
                  control = glmerControl(optimizer = "bobyqa"), 
                  offset = logit(1/numPicN),
                  nAGQ = 1,
                  family = binomial,
                  data = filter(df_expt3, trial_category == "test", block == "test",
                                experiment == "replication", include_good_rt == "include")
                  )

beta_rel_subj_expt3 <- round(summary(m3_expt3)$coef[2], 2)
beta_trialtype_rel_subj_expt3 <- round(summary(m3_expt3)$coef[3], 2)
beta_rel_subj_trialtype_int_expt3 <- round(summary(m3_expt3)$coef[4], 2)

beta_rel_subj_trialtype_int_expt3_p <- round(summary(m3_expt3)$coef[4,4], 2)
```

Figure 4 shows performance on test trials in Experiment 3. Across all conditions for both Trial Types participants selected the correct referent at rates greater than chance (smallest $\beta$ = `r round(sort(e3.chance.tests$betas)[1], 2)`, z = `r round(sort(e3.chance.tests$zs)[1], 2)`, p < .001). Our primary prediction in this experiment is an interaction between reliability and test trial type, with higher levels of reliability leading to less attention and memory allocated to alternative word-object links. To test this prediction, we performed three complementary analyses of test trial performance using three different predictors: reliability condition, participants' use of gaze, and participants' subjective reliablity assessment. 

```{r expt3-table, echo = F, results = 'asis'}
e3.condition <- as.data.frame(summary(m1_expt3)$coef)[4,]
e3.exposure.correct <- as.data.frame(summary(m2_expt3)$coef)[4,]
e3.subj.rel <- as.data.frame(summary(m3_expt3)$coef)[4,]

e3.tab <- bind_rows(e3.condition, e3.exposure.correct, e3.subj.rel)

e3.tab$Predictor <- c("Gaze*Reliablity Condition",
                      "Gaze*Gaze Following on Exposure",
                      "Gaze*Subjective Reliability")

rownames(e3.tab) <- NULL
e3.tab <- e3.tab[,c(5,1:4)]
names(e3.tab)[4:5] <- c("$z$ value","$p$ value")

e3.tab %<>% 
    mutate(
        stars = ifelse(`$p$ value` > .1, "", 
                       ifelse(`$p$ value` < .001, "***",
                              ifelse(`$p$ value` < .01, "**",
                                     ifelse(`$p$ value` < .05, "*",
                                            ifelse(`$p$ value` < .1, ".", "Error"))))),
        `$p$ value` = ifelse(`$p$ value` > .1, round(`$p$ value`, 2), 
                             ifelse(`$p$ value` < .001, "$<$ .001",
                                    ifelse(`$p$ value` < .01, round(`$p$ value`, 2),
                                           ifelse(`$p$ value` < .05, round(`$p$ value`, 2),
                                                  ifelse(`$p$ value` < .1, round(`$p$ value`, 2), 
                                                         "Error")))))
    )


names(e3.tab)[6] <- c("")

# print(xtable(e3.tab,
#              align = c("l","l","r","r","r","r","l"),
#              label = "tab:exp3_reg",
#              caption = "Interaction coefficient estimates with standard errors and significance information for the three logistic mixed-effects models predicting word learning in Experiment 3."),
#       include.rownames=FALSE,hline.after=c(0,nrow(e3.tab)),
#       sanitize.text.function=function(x){x},
#       caption.placement = 'bottom', 
#       table.placement = "tb",
#       comment = F)
```

#### Reliability condition analysis

Panel A of Figure 4 shows participants' accuracy on both types of test trials as a function of the reliablity manipulation. We fit a mixed-effects logistic regression model predicting accuracy using reliability condition as a predictor and found a significant main effect of trial type ($\beta = `r beta_trialtype_expt3`$, p < .001), with lower accuracy on Switch trials. We found a significant interaction between Reliability condition and Trial Type ($\beta = `r beta_rel_trialtype_int_expt3`$, p = `r beta_rel_trialtype_int_p_expt3`), providing evidence for our key prediction. The interaction between Reliability condition and accuracy was relatively weak, however, and -- similar to Experiment 1 -- there was substantial variability across conditions (see the 50% reliable condition in Panel A of Figure 4). To provide additional support for our hypothesis, we conducted two follow-up analyses. 

#### Gaze use analysis

We would only expect to see a strong interaction between reliability and trial type if learners chose to use the gaze cue during exposure trials. To test this hypothesis, we fit a mixed effects logistic regression model with the same specifications, but substituting accuracy on exposure trials for reliability condition as a predictor. We found a robust two-way interaction between accuracy on exposure trials and Trial Type ($\beta = `r beta_totexpo_trialtype_int_expt3`$, p < .001) such that participants who were more likely to use the gaze cue performed worse on Switch trials, but not Same trials. ^[We found this interaction while performing exploratory data analysis on a previous version of this study with an independent sample (N = 250, $\beta = `r beta_totexpo_trialtype_int_expt3_original`$, p < .001). The results reported here are from a follow-up study where testing this interaction was a planned analysis.] Panel B of Figure 4 shows this interaction. 

#### Subjective reliability analysis 

The strong interaction between frequency of gaze use and test trial performance suggests that participants' subjective experience of reliablity in the experiment mattered. To quantify the effect of subjective reliablity, we fit the same mixed effects logistic regression model, but substituted subjective reliability as a predictor of test trial performance. We found a signficant interaction between Trial Type and participants' subjective reliability assessements ($\beta = `r beta_rel_subj_trialtype_int_expt3`$, p = `r beta_rel_subj_trialtype_int_expt3_p`) -- if participants thought the speaker was more reliable, then they performed worse on Switch, but not Same, trials. 

Taken together, these three analyses show that as the speaker's gaze became more reliable, participants were (a) more likely to use it, (b) more likely to rate the speaker as reliable, and (c) less likely to store multiple word-object links. These findings support and extend the results of Experiments 1 and 2 in several important ways. First, participants' performance on Same trials was again relatively unaffected by changes in peformance on Switch trials. The selective effect of gaze on Switch trials provides converging evidence that the limitations on Same trials may be different than those regulating the distribution of attention on Switch trials. Second, learners' *use* of a referential cue was a stronger predictor of reduced memory for alternative word-object links compared to our reliablity manipulation. Although we found a significant effect of reliability on participants' use of the gaze cue, participants' tendency to use the cue remained high. Consider that even in the 0% reliability condition the mean proportion of gaze following was still `r round(min(ms_expt3$accuracy),2)`. It is reasonable that participants would continue to use the gaze cue in our experiment since it was the only cue available and participants did not have a strong reason to think that the speaker would be deceptive. 

The critical contribution of Experiment 3 is that learners respond to a *graded* manipulation of referential uncertainty, with the amount of information stored from the intital exposure tracking with the reliability of the cue and participants' use of the cue. This graded accuracy performance shows that at the group-level learners stored alternative word-object links with different levels of fidelity depending on the amount of referential uncertainty present during learning. 

# General Discussion

Tracking cross-situational word-object statistics allows word learning to proceed despite the presence of individually ambiguous naming events. But models of cross-situational learning disagree about how much information is actually stored in memory and how to best characterize the input to statistical learning mechanisms. In the current line of work, we explore the hypothesis that these two factors are fundamentally linked, both to one another and to the social context in which word learning occurs. Specifically, we ask how cross-situational learning operates over social input that varies along a continuum from low to high ambiguity. 

Our results suggest that the representations underlying cross-situational learning are quite flexible. In the absence of a referential cue to word meaning, learners were more likely to store alternative word-object links. In contrast, when gaze was present, they stored less information, showing behavior consistent with tracking a single hypothesis (Experiments 1 and 2). Learners were also sensitive to a parametric manipulation of the referential cue, showing a graded increase in the tendency to use the cue as reliability increased, which in turn resulted in a graded decrease in memory for alternative word-object links (Experiment 3). Across all three experiments, reduced memory for alternative hypotheses did not result in a boost in memory for learners' candidate hypothesis. This pattern of data suggests that the presence of a referential cue selectively affected one component of the underlying representation: the number of alternative word-object links, and not learners candidate hypothesis.

## Relationship to previous work

Why did we not see an increase in the strength of learners' candidate hypothesis? One possibility is that participants did not shift their cognitive resources from the set of alternatives to their single hypothesis, but instead rationally conserved their resources for future use. @griffiths2015rational formalize this behavior by pushing the rationality of computational-level models down to the psychological process level. In their framework, cognitive systems are thought to be adaptive in that they optimize the use of their limited resources, taking the cost of computation (e.g., opportunity cost of time or mental opportunity) into account. For example, @vul2014 showed that as time pressure increased in a decision-making task, participants were more likely to show behavior consistent with a less cognitively challenging strategy of matching, rather than with the globally optimal strategy. In the current work, we found that learners showed evidence of changing how they allocated cognitive resources based on the amount of referential uncertainty, spending less time studying alternative word-object links and reducing the number of links stored in memory when uncertainty was low.

Our results also fit well with recent experimental work that investigates how attention and memory can constrain infants' statistical word learning. For example, @smith2013visual used a modified cross-situational learning task to show that only infants who disengaged from a novel object to look at both potential referents were able to learn the correct word–object mappings. Moreover, @vlach2013memory showed that 16-month-olds were only able to learn from adjacent cross-situational co-occurrence statistics, and unable to learn from co-occurrences that were separated in time. Both of these findings make the important point that only the information that comes into contact with the learning system can be used for cross-situational word learning, and this information is directly influenced by the attention and memory constraints of the learner. Our findings suggest that referential cues could play an important role in constraining the input to statistical learning mechansims.

How should we characterize the effect of social information on attention and memory in our task? One possibility is that the referential cue acts as a filter, only allowing likely referents to contact statistical learning mechansims [@yu2007unified]. This 'filtering account' separates the effect of social cues from the underlying computation that aggregates cross-situational information. Another possibility is that referential cues provide evidence about a speaker's communicative intent [@frank2009using]. In this model, the learner is reasoning about the speaker and word meanings simultaneously, which places inferences based on social information as part of the underlying computation. A third possibility is that participants thought of the referential cue as pedagogical. In this scenario, learners assume that the speaker will choose an action that is most likely to increase the learner's belief in the true state of the world [@shafto2012learning], making it unnecessary to allocate resources to alternative hypotheses. Experiments show that children spend less time exploring an object and are less likely to discover alternative object-functions, if a single function is demonstrated in a pedagogical context [@bonawitz2011double]. However, because the results from the current study cannot distinguish between these explanations, these questions remain topics for future studies specifically designed to tease apart these possibilities. 

## Limitations

There are several limitations to the current study that are worth noting. First, the social context we used was relatively impoverished. Although we moved beyond a simple manipulation of the presence or absence of social information, we isolated just a single cue to reference, gaze. But real-world learning contexts are much more complex, providing learners access to multiple cues such as gaze, pointing, and previous discourse. In fact, @frank2013social analyzed a corpus of parent-child interactions and concluded that learners would do better to aggregate noisy social information from multiple cues, rather than monitor a single cue, because no single cue was a consistent predictor of reference in their corpus. In our data, we did see a more reliable effect of referential cues when we used a live actress, which included both gaze and head turn as opposed to the static, schematic stimuli, which only included gaze. It is still an open and interesting question as to how our results would generalize to real-world learning environments that contain a rich combination of social cues.

Second, we do not yet know how these results would generalize to young word learners. Research with infants' shows rapid development of visual attention and memory in the first years of life [@ross2003development; @colombo2001development]. Moreover, experimental work shows that infants' attention is often stimulus driven and sticky [@oakes2011infant], suggesting that very young word learners might not effectively explore the visual scene to extract the necessary statistics for effective cross-situational word learning. The current work suggests that referential cues might play an even more important role for young learners, guiding them to the relevant statistics in the input.

And third, in the current experiments we tested a minimal cross-situational learning scenario. Our task contained only one exposure for each novel word-object pairing. In contrast, real world naming events are best characterized by discourse, where an object is likely to be named repeatedly in a short amount of time [@frank2013social; @rohde2014markers]. Moreover, we presented novel words in isolation, removing any sentential cues to word meaning (e.g., verb-argument relations). Previous work shows that sentence-level constraints do interact with cross-situational word learning mechansims [@koehne2014interplay]. We need more evidence to understand how representations underlying cross-situational learning change in response to referential uncertainty at different timescales and in richer language contexts that more accurately reflect learning environments. 

## Conclusions

Word learning proceeds despite the potential for high levels of referential uncertainty and learners’ limited cognitive resources. Our work shows that cross-situational learners flexibly respond to the amount of ambiguity in the input, and as referential uncertainty increases, learners store more word-object links. Overall, these results bring together aspects of both social and statistical accounts of word learning, and increase our understanding of how statistical learning mechanisms operate over fundamentally social input.

\newpage

# Acknowledgements

We are grateful to Rose Schneider for helping record stimuli and to the members of the Language and Cognition Lab for their feedback on this project This work was supported by a National Science Foundation Graduate Research Fellowship to KM, an NIH NRSA Postdoctoral Fellowship to DY, and a John Merck Scholars Fellowship to M.C.F. 

\newpage

# References 

---
nocite: | 
  @smith2014unrealized
...
